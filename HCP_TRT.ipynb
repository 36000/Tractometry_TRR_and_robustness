{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import itertools\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hcp keys from ~/.aws/credentials\n",
    "# you will need to get access to hcp data in order to run this script.\n",
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "CP.sections()\n",
    "aws_access_key = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be run on the cloud for each subject, session, and configuration\n",
    "def afq_hcp_retest(subject, shell, session, seg_algo, reuse_tractography, use_callosal, aws_access_key, aws_secret_key):\n",
    "    import logging\n",
    "    import s3fs\n",
    "\n",
    "    from AFQ.data import fetch_hcp\n",
    "    import AFQ.api as api\n",
    "    import AFQ.mask as afm\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    log = logging.getLogger(__name__)\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    my_hcp_key = \"my_bucket/hcp_trt\"\n",
    "\n",
    "    # get HCP data for the given subject / session\n",
    "    _, hcp_bids = fetch_hcp(\n",
    "        [subject],\n",
    "        profile_name=False,\n",
    "        study=f\"HCP_{session}\",\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "    # if use_callosal, use the callosal bundles\n",
    "    if use_callosal:\n",
    "        bundle_info = api.BUNDLES + api.CALLOSUM_BUNDLES\n",
    "    else:\n",
    "        bundle_info = None\n",
    "\n",
    "    # if single shell, only use b values between 990 and 1010\n",
    "    if \"single\" in shell.lower():\n",
    "        tracking_params = {\"odf_model\": \"DTI\"}\n",
    "        kwargs = {\n",
    "            \"min_bval\": 990,\n",
    "            \"max_bval\": 1010\n",
    "        }\n",
    "    # if multi shell, use DKI instead of CSD everywhere\n",
    "    else:\n",
    "        tracking_params = {\n",
    "            'seed_mask': afm.ScalarMask('dki_fa'),\n",
    "            'stop_mask': afm.ScalarMask('dki_fa'),\n",
    "            \"odf_model\": \"DKI\"}\n",
    "        kwargs = {\n",
    "            \"scalars\": [\"dki_fa\", \"dki_md\"]\n",
    "        }\n",
    "\n",
    "    # use csd if csd is in shell\n",
    "    if \"csd\" in shell.lower():\n",
    "        tracking_params[\"odf_model\"] = \"CSD\"\n",
    "\n",
    "    # Whether to reuse a previous tractography that has already been uploaded to s3\n",
    "    # by another run of this function. Useful if you want to try new parameters that\n",
    "    # do not change the tractography.\n",
    "    if reuse_tractography:\n",
    "        fs.get(\n",
    "            (\n",
    "                f\"{my_hcp_key}/{shell}_shell/\"\n",
    "                f\"hcp_{session.lower()}_afq/sub-{subject}/ses-01/\"\n",
    "                f\"sub-{subject}_dwi_space-RASMM_model-\"\n",
    "                f\"{tracking_params['odf_model']}_desc-det_tractography.trk\"),\n",
    "            op.join(hcp_bids, f\"derivatives/dmriprep/sub-{subject}/ses-01/sub-{subject}_customtrk.trk\"))\n",
    "        custom_tractography_bids_filters = {\n",
    "            \"suffix\": \"customtrk\", \"scope\": \"dmriprep\"}\n",
    "    else:\n",
    "        custom_tractography_bids_filters = None\n",
    "\n",
    "    # Initialize the AFQ object with all of the parameters we have set so far\n",
    "    # Also uses the brain mask provided by HCP\n",
    "    # Sets viz_backend='plotly' to make GIFs in addition to the default\n",
    "    # html visualizations (this adds ~45 minutes)\n",
    "    myafq = api.AFQ(\n",
    "        hcp_bids,\n",
    "        brain_mask=afm.LabelledMaskFile('seg', {'scope':'dmriprep'}, exclusive_labels=[0]),\n",
    "        custom_tractography_bids_filters=custom_tractography_bids_filters,\n",
    "        tracking_params=tracking_params,\n",
    "        bundle_info=bundle_info,\n",
    "        segmentation_params={\"seg_algo\": seg_algo, \"reg_algo\": \"syn\"},\n",
    "        viz_backend='plotly',\n",
    "        **kwargs)\n",
    "    # run the AFQ objects\n",
    "    myafq.export_all()\n",
    "\n",
    "    # upload the results to my_hcp_key, organized by parameters used\n",
    "    remote_export_path = f\"{my_hcp_key}/{shell}_shell/hcp_{session.lower()}_{seg_algo}\"\n",
    "    if use_callosal:\n",
    "        remote_export_path = remote_export_path + \"_callosal\"\n",
    "    myafq.upload_to_s3(fs, remote_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the HCP subjects with DWI data in the HCP test-retest dataset (HCP-TRT)\n",
    "all_subjects = [\n",
    "    103818,\n",
    "    105923,\n",
    "    111312,\n",
    "    114823,\n",
    "    115320,\n",
    "    122317,\n",
    "    125525,\n",
    "    130518,\n",
    "    135528,\n",
    "    137128,\n",
    "    139839,\n",
    "    143325,\n",
    "    144226,\n",
    "    146129,\n",
    "    149337,\n",
    "    149741,\n",
    "    151526,\n",
    "    158035,\n",
    "    169343,\n",
    "    172332,\n",
    "    175439,\n",
    "    177746,\n",
    "    185442,\n",
    "    187547,\n",
    "    192439,\n",
    "    194140,\n",
    "    195041,\n",
    "    200109,\n",
    "    200614,\n",
    "    204521,\n",
    "    250427,\n",
    "    287248,\n",
    "    341834,\n",
    "    433839,\n",
    "    562345,\n",
    "    599671,\n",
    "    601127,\n",
    "    627549,\n",
    "    660951,\n",
    "    783462,\n",
    "    859671,\n",
    "    861456,\n",
    "    877168,\n",
    "    917255\n",
    "]\n",
    "len(all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [str(i) for i in all_subjects] # converts subject numbers to strings\n",
    "shell = [\"multi\", \"single\", \"CSD\"] # try different ODF models for tractography\n",
    "session = [\"1200\", \"Retest\"] # use both the test and retest datasets\n",
    "seg_algo = [\"afq\", \"reco80\"] # try dfferent segmentation algorithms\n",
    "reuse_tractography = [False] # Set to True to reuse tractography from a previous run with the same shell\n",
    "use_callosal = [False] # set to True to get callosal bundles\n",
    "\n",
    "# try all different combinations of subjects, shells, sessoins, and other parameters\n",
    "args = list(itertools.product(subjects, shell, session, seg_algo, reuse_tractography, use_callosal))\n",
    "print(args)\n",
    "\n",
    "# attach aws keys to each list of arguments\n",
    "def attach_keys(list_of_arg_lists):\n",
    "    new_list_of_arg_lists = []\n",
    "    for args in list_of_arg_lists:\n",
    "        arg_ls = list(args)\n",
    "        arg_ls.extend([aws_access_key, aws_secret_key])\n",
    "        new_list_of_arg_lists.append(arg_ls)\n",
    "    return new_list_of_arg_lists\n",
    "args = attach_keys(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cloudknot to process HCP on aws\n",
    "# will require your own aws account with access to s3 and ec2 resources\n",
    "# credentials should be in ~/.aws/credentials with the hcp credentials\n",
    "import cloudknot as ck\n",
    "ck.set_region('us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and push a docker image with non-conflicting requirements that uses version 0.6 of pyafq\n",
    "di = ck.DockerImage(\n",
    "    name='hcp-api-jk',\n",
    "    func=afq_hcp_retest,\n",
    "    base_image=\"python:3.8\",\n",
    "    github_installs='https://github.com/yeatmanlab/pyAFQ.git@0.6',\n",
    "    overwrite=True)\n",
    "with open(di.req_path, \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"pandas==1.1.4\n",
    "        nibabel==3.2.1\n",
    "        boto3==1.14.18\n",
    "        s3fs==0.5.1\n",
    "        dipy==1.3.0\n",
    "        cloudpickle==1.6.0\"\"\")\n",
    "di.build(tags=[\"hcp-trt-210101-0\"])\n",
    "repo = ck.aws.DockerRepo(name=ck.get_ecr_repo())\n",
    "di.push(repo=repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make knot to process HCP data, which is high quality and requires a lot of memory and disk space\n",
    "knot = ck.Knot(\n",
    "    name='hcp-trt-210101-0',\n",
    "    docker_image=di,\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    bid_percentage=100,\n",
    "    volume_size=64,\n",
    "    max_vcpus=256,\n",
    "    memory=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the first 3 argument combinations on cloud (do the rest if these succeed)\n",
    "result_futures = knot.map(args[:3], starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status of your jobs\n",
    "ck.set_region('us-west-2')\n",
    "knot.view_jobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status of and individual job\n",
    "j0 = knot.jobs[0]\n",
    "j0.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clobber your knot resource when you are done\n",
    "#knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AFQ.api as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when your done, download and combine afq profiles from all of the various jobs\n",
    "\n",
    "# DTI, single shell\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/single_shell_test_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/single_shell/hcp_1200_afq\"\n",
    "    )\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/single_shell_retest_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/single_shell/hcp_retest_afq\"\n",
    "    )\n",
    "\n",
    "# DKI, multi shell\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_test_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_1200_afq\"\n",
    "    )\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_retest_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_retest_afq\"\n",
    "    )\n",
    "\n",
    "# reco80, DKI, multi shell\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_test_reco80_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_1200_reco80\"\n",
    "    )\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_retest_reco80_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_retest_reco80\"\n",
    "    )\n",
    "\n",
    "# CSD, multi shell\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_CSD_test_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_1200_afq_CSD\"\n",
    "    )\n",
    "api.download_and_combine_afq_profiles(\n",
    "    \"~/AFQ_data/hcp_reliability_profiles/multi_shell_CSD_retest_profiles.csv\",\n",
    "    \"my_bucket\",\n",
    "    \"hcp_trt/multi_shell/hcp_retest_afq_CSD\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get json file for RTP results from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7705748/\n",
    "import pandas as pd\n",
    "data_path = op.join(op.expanduser(\"~\"), \"AFQ_data/hcp_reliability_profiles\", \"AllV04_multiSiteAndMeas_ComputationalReproducibility_2020-08-03T12-36.json\")\n",
    "hcp_ready = pd.read_json(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert format from RTP to our format (takes ~10-15 minutes)\n",
    "from tqdm import tqdm\n",
    "hcp_ready = hcp_ready[hcp_ready[\"Proj\"] == \"HCP\"]\n",
    "hcp_ready = hcp_ready.rename(columns={\"Struct\": \"tractID\", \"SubjID\": \"subjectID\"})\n",
    "hcp_ready = hcp_ready.drop([\"Proj\", \"ad\", \"curvature\", \"rd\", \"torsion\", \"volume\", \"SubjectMD\", \"AcquMD\", \"AnalysisMD\"], axis=1)\n",
    "hcp_ready_test = pd.DataFrame(columns=[\"nodeID\", \"tractID\", \"subjectID\", \"fa\", \"md\"])\n",
    "hcp_ready_retest = pd.DataFrame(columns=[\"nodeID\", \"tractID\", \"subjectID\", \"fa\", \"md\"])\n",
    "with tqdm(total=hcp_ready.shape[0]) as pbar:\n",
    "    for index, row in hcp_ready.iterrows():\n",
    "        pbar.update(1)\n",
    "        if row[\"Proj\"] == \"HCP\":\n",
    "            if row[\"AcquMD\"][0]['scanbValue'] == 1000:\n",
    "                if row[\"TRT\"] == \"TEST\":\n",
    "                    for i in range(100):\n",
    "                        hcp_ready_test = hcp_ready_test.append({\n",
    "                            \"nodeID\": i,\n",
    "                            \"tractID\": row[\"Struct\"],\n",
    "                            \"subjectID\": row[\"SubjID\"],\n",
    "                            \"fa\": row[\"fa\"][i],\n",
    "                            \"md\": row[\"md\"][i]},\n",
    "                            ignore_index=True)\n",
    "                elif row[\"TRT\"] == \"RETEST\":\n",
    "                    for i in range(100):\n",
    "                        hcp_ready_retest = hcp_ready_retest.append({\n",
    "                            \"nodeID\": i,\n",
    "                            \"tractID\": row[\"Struct\"],\n",
    "                            \"subjectID\": row[\"SubjID\"],\n",
    "                            \"fa\": row[\"fa\"][i],\n",
    "                            \"md\": row[\"md\"][i]},\n",
    "                            ignore_index=True)\n",
    "                else:\n",
    "                    raise ValueError(\"TRT not test or retest\")\n",
    "hcp_ready_test.to_csv(\"~/AFQ_data/hcp_reliability_profiles/rtp_test_profiles.csv\", index=False)\n",
    "hcp_ready_retest.to_csv(\"~/AFQ_data/hcp_reliability_profiles/rtp_retest_profiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AFQ.viz.utils as vut\n",
    "import logging\n",
    "from importlib import reload\n",
    "import AFQ.data as afd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupCSVComparison objects contain a list of different CSVs to compare\n",
    "# and methods for doing the comparison (ie: tract profiles, ACIPs, subject and profile reliabilities)\n",
    "\n",
    "reload(vut) # reloads viz.utils (useful after making tweaks to plotting code)\n",
    "waypoint_comparisons = vut.GroupCSVComparison( # comparisons using waypoint ROI bundles\n",
    "    'hcp_reliability_profiles/comparisons',\n",
    "    [\n",
    "        \"~/hcp_reliability_profiles/single_shell_test_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/single_shell_retest_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/multi_shell_test_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/multi_shell_retest_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/multi_shell_CSD_test_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/multi_shell_CSD_retest_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/rtp_test_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/rtp_retest_profiles.csv\"\n",
    "    ], [\n",
    "        'single_test',\n",
    "        'single_retest',\n",
    "        'multi_test',\n",
    "        'multi_retest',\n",
    "        'multi_CSD_test',\n",
    "        'multi_CSD_retest',\n",
    "        'rtp_test',\n",
    "        'rtp_retest'\n",
    "    ], is_special = [\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        'mat',\n",
    "        'mat'\n",
    "    ],\n",
    "    subjects=all_subjects,\n",
    "    remove_model=True,\n",
    "    scalar_bounds={'lb': {'FA': 0.2},\n",
    "                   'ub': {'MD': 0.002}})\n",
    "waypoint_comparisons.logger.setLevel(logging.WARNING)\n",
    "\n",
    "reco_waypoint_comparison = vut.GroupCSVComparison( # comparisons on overlap between reco and afq bundles\n",
    "    'hcp_reliability_profiles/comparisons',\n",
    "    [\n",
    "        \"~/hcp_reliability_profiles/multi_shell_test_reco80_profiles.csv\",\n",
    "        \"~/hcp_reliability_profiles/multi_shell_test_profiles.csv\",\n",
    "    ],\n",
    "    ['multi_test_reco80', 'multi_test'],\n",
    "    subjects=all_subjects, bundles=list(afd.BUNDLE_RECO_2_AFQ.values()),\n",
    "    is_special = [\"reco\", \"\"])\n",
    "reco_waypoint_comparison.logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tract profile\n",
    "waypoint_comparisons.tract_profiles(names=['single_test'], show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HCP TRR, multi shell vs csd\n",
    "_, _, _, suf_bundles, multi_intersubject, _, multi_profile, _ = \\\n",
    "    waypoint_comparisons.reliability_plots(\n",
    "        names=['multi_test', 'multi_retest'], show_plots=True)\n",
    "_, _, _, _, CSD_intersubject, _, CSD_profile, _ = \\\n",
    "    waypoint_comparisons.reliability_plots(\n",
    "        names=['multi_CSD_test', 'multi_CSD_retest'], show_plots=True)\n",
    "\n",
    "waypoint_comparisons.compare_reliability(\n",
    "    CSD_profile, multi_profile, \"CSD\", \"DKI\", suf_bundles, rtype=\"Profile TRR\", show_plots=True, show_legend=False)\n",
    "waypoint_comparisons.compare_reliability(\n",
    "    CSD_intersubject, multi_intersubject, \"CSD\", \"DKI\", suf_bundles, rtype=\"Subject TRR\", show_plots=True, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCP TRR, single shell vs RTP\n",
    "_, _, _, _, single_intersubject, _, single_profile, _ = \\\n",
    "    waypoint_comparisons.reliability_plots(\n",
    "        names=['single_test', 'single_retest'], show_plots=True)\n",
    "_, _, _, _, ready_intersubject, _, ready_profile, _ = \\\n",
    "    waypoint_comparisons.reliability_plots(\n",
    "        names=['rtp_test', 'rtp_retest'], show_plots=True)\n",
    "\n",
    "waypoint_comparisons.compare_reliability(\n",
    "    single_profile, ready_profile, \"pyAFQ DTI\", \"RTP DTI\", suf_bundles, rtype=\"Profile TRR\", show_plots=True, show_legend=False)\n",
    "waypoint_comparisons.compare_reliability(\n",
    "    single_intersubject, ready_intersubject, \"pyAFQ DTI\", \"RTP DTI\", suf_bundles, rtype=\"Subject TRR\", show_plots=True, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCP ODF robustness\n",
    "waypoint_comparisons.reliability_plots(\n",
    "        names=['multi_test', 'multi_CSD_test'], scalars=[\"FA\", \"MD\"], rtype=\"Robustness\", show_plots=True)\n",
    "waypoint_comparisons.contrast_index(names=[\"multi_test\", \"multi_CSD_test\"], show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCP Recobundles robustness\n",
    "reco_waypoint_comparison.contrast_index(show_plots=True)\n",
    "reco_waypoint_comparison.reliability_plots(rtype=\"Robustness\", show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
